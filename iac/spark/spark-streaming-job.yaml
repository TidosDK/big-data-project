apiVersion: batch/v1
kind: Job
metadata:
  name: spark-stream-legacy-hybrid
  namespace: bd-bd-gr-02
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: spark-streaming
          image: bitnami/spark:3.5.2-debian-12-r1
          imagePullPolicy: IfNotPresent

          # We need to expose the Pod IP for the Spark Driver to work
          env:
            - name: SPARK_DRIVER_HOST
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # Ensure we can write the temporary file
            - name: HOME
              value: /tmp

          securityContext:
            runAsUser: 0

          command: ["/bin/bash", "-c"]
          args:
            - |
              echo "Starting Legacy Streaming (Patched)..."
              
              # Submit the job with the necessary Kafka JARs.
              # The Python script will handle the missing wrapper.
              /opt/bitnami/spark/bin/spark-submit \
              --master spark://spark-master-0.spark-headless.bd-bd-gr-02.svc.cluster.local:7077 \
              --deploy-mode client \
              --name energy-weather-legacy-final \
              --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2 \
              --conf "spark.driver.bindAddress=$(SPARK_DRIVER_HOST)" \
              --conf "spark.driver.host=$(SPARK_DRIVER_HOST)" \
              --conf "spark.executor.instances=2" \
              --conf "spark.executor.memory=2g" \
              --conf "spark.driver.memory=1g" \
              /opt/spark/app/streaminglegacy.py

          volumeMounts:
            - name: app
              mountPath: /opt/spark/app
      volumes:
        - name: app
          configMap:
            name: spark-streaming-configmap