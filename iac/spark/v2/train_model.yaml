apiVersion: batch/v1
kind: Job
metadata:
  name: model-training-job
  namespace: bd-bd-gr-02
spec:
  backoffLimit: 0
  template:
    spec:
      restartPolicy: Never
      containers:
        - name: spark-trainer
          image: bitnami/spark:3.5.2-debian-12-r1
          securityContext:
            runAsUser: 0
          env:
            - name: PYTHONPATH
              value: "/tmp/pyspark_lib:/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip:/opt/bitnami/spark/python"
          command: ["/bin/bash", "-c"]
          args:
            - |
              echo ">>> Installing numpy..."
              pip install numpy --target=/tmp/pyspark_lib
              
              echo ">>> Creating Kafka-Training Script..."
              cat <<EOF > /tmp/train_kafka.py
              import sys
              sys.path.insert(0, "/tmp/pyspark_lib")
              
              from pyspark.sql import SparkSession
              from pyspark.sql.functions import col, from_json, to_timestamp, hour, dayofmonth, month, year, dayofweek
              from pyspark.sql.types import StructType, StructField, StringType, DoubleType
              from pyspark.ml import Pipeline
              from pyspark.ml.feature import VectorAssembler
              from pyspark.ml.regression import LinearRegression
              
              spark = SparkSession.builder.appName("KafkaTrainer").getOrCreate()
              spark.sparkContext.setLogLevel("WARN")

              print(">>> Reading from Kafka 'energy_data' (Earliest)...")
              
              # 1. READ KAFKA (Batch Mode)
              # We read the entire topic history as a static DataFrame
              raw_kafka_df = spark.read \
                  .format("kafka") \
                  .option("kafka.bootstrap.servers", "kafka:9092") \
                  .option("subscribe", "energy_data") \
                  .option("startingOffsets", "earliest") \
                  .load()
              
              # 2. PARSE JSON
              # Schema must match your producer's format
              json_schema = StructType([
                  StructField("TimeUTC", StringType(), True),
                  StructField("RegionName", StringType(), True),
                  StructField("ConsumptionkWh", DoubleType(), True)
              ])
              
              parsed_df = raw_kafka_df.select(
                  from_json(col("value").cast("string"), json_schema).alias("data")
              ).select("data.*")
              
              # 3. FEATURE ENGINEERING
              print(">>> Transforming Data for Training...")
              train_df = parsed_df \
                  .withColumn("ts", to_timestamp(col("TimeUTC"))) \
                  .withColumn("Year", year(col("ts")).cast("double")) \
                  .withColumn("Month", month(col("ts")).cast("double")) \
                  .withColumn("Day", dayofmonth(col("ts")).cast("double")) \
                  .withColumn("Hour", hour(col("ts")).cast("double")) \
                  .withColumn("DayOfWeek", dayofweek(col("ts")).cast("double")) \
                  .withColumn("ConsumptionkWh", col("ConsumptionkWh").cast("double")) \
                  .na.drop() # Drop rows with bad/missing data
              
              # Check if we actually got data
              count = train_df.count()
              print(f"DEBUG: Found {count} valid training records from Kafka.")
              
              if count == 0:
                  print("ERROR: No data found in Kafka topic! Cannot train model.")
                  sys.exit(1)
              
              train_df.show(5)
              
              # 4. TRAIN MODEL
              print(">>> Training Linear Regression Model...")
              assembler = VectorAssembler(
                  inputCols=["Year", "Month", "Day", "Hour", "DayOfWeek"], 
                  outputCol="features"
              )
              
              lr = LinearRegression(featuresCol="features", labelCol="ConsumptionkWh")
              pipeline = Pipeline(stages=[assembler, lr])
              
              model = pipeline.fit(train_df)
              
              # 5. SAVE MODEL
              output_path = "/data/energy_prediction_model"
              model.write().overwrite().save(output_path)
              print(f"SUCCESS: Model trained on {count} records and saved to {output_path}")
              EOF
              
              echo ">>> Running Spark Submit..."
              # Note: We include the Kafka package here too!
              /opt/bitnami/spark/bin/spark-submit \
              --master local[1] \
              --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2 \
              /tmp/train_kafka.py

          volumeMounts:
            - name: data-volume
              mountPath: /data
      volumes:
        - name: data-volume
          persistentVolumeClaim:
            claimName: spark-streaming-pvc